# -*- coding: utf-8 -*-
"""SPARK_SQL_2001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J5U3fhj5aJioH3oSI-SUry7zfbWOyUsS
"""

#Цель: изучить предложенный датасет по заболеваемости ковид-19 и получить навык работы с DataFrame API

from pyspark.sql import functions as F
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("COVID Data Analysis").getOrCreate()
from pyspark.sql.functions import round

df = spark.read.csv("/content/drive/MyDrive/Colab Notebooks/covid-data.csv", header=True, inferSchema=True)
df.show(5)

#Выберите 15 стран с наибольшим процентом переболевших на 31 марта (в выходящем датасете необходимы колонки: iso_code, страна, процент переболевших)
#total_cases - всего случаев
#Переболевшие - это? всего случаев от общей популяции?
#Округлить до 2 знаков

# Фильтруем данные по 31 марта 2021 года
df_march_31 = df.filter(df["date"] == "2021-03-31")

# Рассчитываем процент переболевших
df_1 = df_march_31.withColumn("recovered_percentage", round((df_march_31["total_cases"]/ df_march_31["population"]) * 100,2))

# Выбираем нужные столбцы и сортируем по убыванию и выбираем топ-15
df_top = df_1.select("iso_code", "location", "recovered_percentage") \
                      .orderBy(F.col("recovered_percentage").desc()) \
                      .limit(15)
df_top.show()

#Top 10 стран с максимальным зафиксированным кол-вом новых случаев за последнюю неделю марта 2021 в отсортированном порядке по убыванию
#(в выходящем датасете необходимы колонки: число, страна, кол-во новых случаев)
#В выборку попадают континенты и мир в целом - это ок?

# Фильтруем данные за неделю марта 2021
df_march_25_31 = df.filter(df["date"].between("2021-03-25", "2021-03-31"))

# Группируем по стране и дате и находим максимальное  количество новых случаев за неделю
df_new_cases_grouped = df_march_25_31.groupBy("date", "location").agg(F.max("new_cases").alias("total_new_cases"))
#df_new_cases_grouped.show()
df_max_day_per_country = df_new_cases_grouped.groupBy("location").agg(F.max("total_new_cases").alias("max_new_cases"))
#df_max_day_per_country.show()
#Убираем наименования континентов и мир.
df_corr = df_max_day_per_country.filter(~df_max_day_per_country["location"].isin("World", "Europe", "European Union", "Asia", "Brazil","North America", "South America"))
# Сортируем по убыванию по количеству новых случаев и выводим 10 с максимальным количеством новых случаев.
df_top_10_new_cases = df_corr.orderBy(F.col("max_new_cases").desc()).limit(10)
#df_top_10_new_cases.show()
# Объединяем максимальное количество новых случаев с датой.
df_final = df_new_cases_grouped.join(df_top_10_new_cases, on="location", how="inner")
#Убираем повторения стран
df_final_unique = df_final.dropDuplicates(["location"])
#Сортируем и убираем столбец total_new_cases
df_fin_sort=df_final_unique.orderBy(F.col("max_new_cases").desc())
df_finita=df_fin_sort.select("location","date","max_new_cases")
df_finita.show()

#Посчитайте изменение случаев относительно предыдущего дня в России за последнюю неделю марта 2021.
 #(например: в россии вчера было 9150 , сегодня 8763, итог: -387)
 #(в выходящем датасете необходимы колонки: число, кол-во новых случаев вчера, кол-во новых случаев сегодня, дельта)

from pyspark.sql import functions as F
from pyspark.sql.window import Window
#Поменяла порядок действий согласно замечаниям к работе. Теперь нет NULL в первой строке.
# Сортируем данные
window_spec = Window.orderBy("date")

#Рассчитываем дельту
df_delta = df.withColumn("previous_day_new_cases", F.lag("new_cases").over(window_spec)) \
                                 .withColumn("delta", df["new_cases"] - F.col("previous_day_new_cases"))

# Фильтруем данные по России за последнюю неделю марта
df_russia_march = df_delta.filter(df["location"] == "Russia").filter(df["date"].between("2021-03-25", "2021-03-31"))

df_rus_delta=df_russia_march.select("date", "previous_day_new_cases", "new_cases", "delta")
df_rus_delta.show()

df_top.write.csv("/content/drive/MyDrive/Colab Notebooks/path_to_output_top_2001.csv", header=True)
df_finita.write.csv("/content/drive/MyDrive/Colab Notebooks/path_to_output_df_finita_2001.csv", header=True)
df_rus_delta.write.csv("/content/drive/MyDrive/Colab Notebooks/path_to_output_df_rus_delta_2001.csv", header=True)