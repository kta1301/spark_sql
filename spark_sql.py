# -*- coding: utf-8 -*-
"""SPARK_SQL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J5U3fhj5aJioH3oSI-SUry7zfbWOyUsS
"""

#Цель: изучить предложенный датасет по заболеваемости ковид-19 и получить навык работы с DataFrame API

from pyspark.sql import functions as F

df = spark.read.csv("/content/drive/MyDrive/Colab Notebooks/covid-data.csv", header=True, inferSchema=True)
df.show(5)

#Выберите 15 стран с наибольшим процентом переболевших на 31 марта (в выходящем датасете необходимы колонки: iso_code, страна, процент переболевших)
#total_cases - всего случаев
#Переболевшие - это? всего случаев от общей популяции?

# Фильтруем данные по 31 марта 2021 года
df_march_31 = df.filter(df["date"] == "2021-03-31")

# Рассчитываем процент переболевших
df_1 = df_march_31.withColumn("recovered_percentage", (df_march_31["total_cases"]/ df_march_31["population"]) * 100)

# Выбираем нужные столбцы и сортируем по убыванию и выбираем топ-15
df_top = df_1.select("iso_code", "location", "recovered_percentage") \
                      .orderBy(F.col("recovered_percentage").desc()) \
                      .limit(15)
df_top.show()

#Top 10 стран с максимальным зафиксированным кол-вом новых случаев за последнюю неделю марта 2021 в отсортированном порядке по убыванию
#(в выходящем датасете необходимы колонки: число, страна, кол-во новых случаев)
#В выборку попадают континенты и мир в целом - это ок?

# Фильтруем данные за неделю марта 2021
df_march_25_31 = df.filter(df["date"].between("2021-03-25", "2021-03-31"))

# Группируем по стране и дате и  считаем суммарное количество новых случаев за неделю
df_new_cases_grouped = df_march_25_31.groupBy("date","location").agg(F.sum("new_cases").alias("total_new_cases"))
# Сортируем по убыванию по количеству новых случаев и выводим 10 с максимальным количеством новых случаев.
df_top_10_new_cases = df_new_cases_grouped.orderBy(F.col("total_new_cases").desc()).limit(10)

df_top_10_new_cases.show()

#Посчитайте изменение случаев относительно предыдущего дня в России за последнюю неделю марта 2021.
 #(например: в россии вчера было 9150 , сегодня 8763, итог: -387)
 #(в выходящем датасете необходимы колонки: число, кол-во новых случаев вчера, кол-во новых случаев сегодня, дельта)

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Фильтруем данные по России за последнюю неделю марта
df_russia_march = df.filter(df["location"] == "Russia").filter(df["date"].between("2021-03-25", "2021-03-31"))

# Сортируем данные
window_spec = Window.orderBy("date")

#Рассчитываем дельту
df_russia_with_delta = df_russia_march.withColumn("previous_day_new_cases", F.lag("new_cases").over(window_spec)) \
                                 .withColumn("delta", df_russia_march["new_cases"] - F.col("previous_day_new_cases"))

df_rus_delta=df_russia_with_delta.select("date", "previous_day_new_cases", "new_cases", "delta")
df_rus_delta.show()

df_top.write.csv("path_to_output.csv", header=True)
df_top_10_new_cases.write.csv("path_to_output_top_10_new_cases.csv", header=True)
df_rus_delta.write.csv("path_to_output_df_rus_delta.csv", header=True)